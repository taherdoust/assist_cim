{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CIM Assistant with Hugging Face Models\n",
        "\n",
        "This notebook uses Hugging Face models instead of Ollama for local inference.\n",
        "Perfect for when your Ollama server is unavailable!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LangChain imports\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
        "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool, InfoSQLDatabaseTool, ListSQLDatabaseTool, QuerySQLCheckerTool\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Hugging Face integration\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "\n",
        "print(\"‚úÖ All imports loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load environment variables\n",
        "load_dotenv()\n",
        "print(\"‚úÖ Environment variables loaded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Selection\n",
        "\n",
        "Choose your preferred model based on your needs:\n",
        "- **Qwen2.5-7B**: Great for coding, reasoning, and SQL tasks\n",
        "- **Mistral-7B**: Excellent for instruction following\n",
        "- **CodeLlama-7B**: Specialized for code generation\n",
        "- **Llama-3.1-8B**: Meta's latest model with great performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Configuration\n",
        "# Change this to your preferred model\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"  # Default choice\n",
        "\n",
        "# Alternative models (uncomment one to use):\n",
        "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "# MODEL_NAME = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
        "# MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "print(f\"Selected model: {MODEL_NAME}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Hugging Face Model\n",
        "print(f\"üîÑ Loading {MODEL_NAME}...\")\n",
        "print(\"This may take a few minutes on first run...\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Load model with memory optimization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,  # Use half precision to save memory\n",
        "    device_map=\"auto\",  # Automatically distribute across available GPUs\n",
        "    trust_remote_code=True,\n",
        "    low_cpu_mem_usage=True  # Reduce CPU memory usage during loading\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Hugging Face Pipeline\n",
        "print(\"üîÑ Creating pipeline...\")\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.0,\n",
        "    do_sample=True,\n",
        "    return_full_text=False,\n",
        "    pad_token_id=tokenizer.eos_token_id  # Handle padding\n",
        ")\n",
        "\n",
        "# Initialize LangChain LLM\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "print(\"‚úÖ Pipeline created and LangChain LLM initialized!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the Hugging Face model\n",
        "print(\"üß™ Testing Hugging Face model...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "test_prompt = \"Hello! Can you help me with SQL queries? Please respond briefly.\"\n",
        "response = llm.invoke(test_prompt)\n",
        "\n",
        "print(f\"Prompt: {test_prompt}\")\n",
        "print(f\"Response: {response}\")\n",
        "print(\"-\" * 50)\n",
        "print(\"‚úÖ Model is working correctly!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Database Connection Setup\n",
        "\n",
        "Now set up your database connection (same as your original notebook)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Database connection setup\n",
        "# Update these connection details for your database\n",
        "DB_USER = \"your_username\"\n",
        "DB_PASSWORD = \"your_password\"\n",
        "DB_HOST = \"localhost\"\n",
        "DB_PORT = \"5432\"\n",
        "DB_NAME = \"your_database\"\n",
        "\n",
        "# Create database URI\n",
        "DATABASE_URI = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
        "\n",
        "print(f\"Database URI: {DATABASE_URI}\")\n",
        "print(\"‚ö†Ô∏è  Please update the database credentials above!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize SQL Database\n",
        "try:\n",
        "    db = SQLDatabase.from_uri(DATABASE_URI)\n",
        "    print(\"‚úÖ Database connection successful!\")\n",
        "    \n",
        "    # Test database connection\n",
        "    tables = db.get_usable_table_names()\n",
        "    print(f\"üìä Available tables: {tables}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Database connection failed: {e}\")\n",
        "    print(\"Please check your database credentials and connection.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SQL Agent Setup\n",
        "\n",
        "Create the SQL agent with your Hugging Face model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create SQL Database Toolkit\n",
        "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
        "\n",
        "# Get tools from toolkit\n",
        "tools = toolkit.get_tools()\n",
        "\n",
        "print(f\"üîß Available SQL tools: {[tool.name for tool in tools]}\")\n",
        "print(\"‚úÖ SQL toolkit created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create SQL Agent\n",
        "agent = create_react_agent(\n",
        "    llm=llm,\n",
        "    tools=tools,\n",
        "    state_modifier=ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"You are a helpful SQL assistant. \n",
        "        You can help users write SQL queries, analyze database schemas, and answer questions about data.\n",
        "        Always be precise and helpful in your responses.\"\"\"),\n",
        "        (\"placeholder\", \"{messages}\")\n",
        "    ])\n",
        ")\n",
        "\n",
        "print(\"‚úÖ SQL Agent created successfully!\")\n",
        "print(\"ü§ñ Your Hugging Face-powered SQL assistant is ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Your Agent\n",
        "\n",
        "Try some SQL queries with your Hugging Face model!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the agent with a simple query\n",
        "test_query = \"What tables are available in the database?\"\n",
        "\n",
        "print(f\"üîç Testing query: {test_query}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "try:\n",
        "    result = agent.invoke({\"messages\": [HumanMessage(content=test_query)]})\n",
        "    print(f\"Response: {result['messages'][-1].content}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory Optimization Options\n",
        "\n",
        "If you're running into memory issues, try these configurations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory optimization configurations\n",
        "\n",
        "# Option 1: 4-bit quantization (requires bitsandbytes)\n",
        "\"\"\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,  # 4-bit quantization\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# Option 2: 8-bit quantization\n",
        "\"\"\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,  # 8-bit quantization\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "# Option 3: CPU-only inference (slower but uses less GPU memory)\n",
        "\"\"\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float32,\n",
        "    device_map=\"cpu\",  # Force CPU usage\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "print(\"üí° Memory optimization options available above.\")\n",
        "print(\"Uncomment the configuration you want to use.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Performance Comparison\n",
        "\n",
        "Here's a quick comparison of different models for SQL tasks:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model recommendations for different tasks\n",
        "model_recommendations = {\n",
        "    \"SQL & Database Tasks\": [\n",
        "        \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "        \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "        \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "    ],\n",
        "    \"Code Generation\": [\n",
        "        \"codellama/CodeLlama-7b-Instruct-hf\",\n",
        "        \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "        \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "    ],\n",
        "    \"General Reasoning\": [\n",
        "        \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "        \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "    ],\n",
        "    \"Low Memory (4-bit)\": [\n",
        "        \"Qwen/Qwen2.5-7B-Instruct\",\n",
        "        \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for task, models in model_recommendations.items():\n",
        "    print(f\"\\nüìã {task}:\")\n",
        "    for i, model in enumerate(models, 1):\n",
        "        print(f\"  {i}. {model}\")\n",
        "\n",
        "print(\"\\nüí° Choose the model that best fits your needs and hardware!\")\n",
        "print(\"üîÑ To switch models, just change the MODEL_NAME variable and re-run the loading cells.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
